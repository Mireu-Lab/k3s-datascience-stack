---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-hadoop-config
  namespace: spark
data:
  core-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfs-namenode.hadoop.svc.cluster.local:9000</value>
      </property>
      <property>
        <name>hadoop.proxyuser.spark.hosts</name>
        <value>*</value>
      </property>
      <property>
        <name>hadoop.proxyuser.spark.groups</name>
        <value>*</value>
      </property>
    </configuration>
  
  hdfs-site.xml: |
    <?xml version="1.0"?>
    <configuration>
      <property>
        <name>dfs.replication</name>
        <value>1</value>
      </property>
      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>true</value>
      </property>
    </configuration>

  spark-defaults.conf: |
    spark.master                     spark://spark-master-svc:7077
    spark.eventLog.enabled           true
    spark.eventLog.dir               hdfs://hdfs-namenode.hadoop.svc.cluster.local:9000/spark-logs
    spark.history.fs.logDirectory    hdfs://hdfs-namenode.hadoop.svc.cluster.local:9000/spark-logs
    spark.sql.warehouse.dir          hdfs://hdfs-namenode.hadoop.svc.cluster.local:9000/spark-warehouse
    spark.hadoop.fs.defaultFS        hdfs://hdfs-namenode.hadoop.svc.cluster.local:9000
    
    # PostgreSQL JDBC 설정
    spark.jars.packages              org.postgresql:postgresql:42.7.1
    
    # Hadoop AWS S3 지원
    spark.hadoop.fs.s3a.impl         org.apache.hadoop.fs.s3a.S3AFileSystem
