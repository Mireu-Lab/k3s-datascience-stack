{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288fb5a9",
   "metadata": {},
   "source": [
    "# PySpark 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e986cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성 (HDFS 연결 포함)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Example\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://hdfs-cluster:8020\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7b2c1",
   "metadata": {},
   "source": [
    "# 데이터프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 데이터 생성 (또는 HDFS에서 읽기)\n",
    "data = [\n",
    "    (\"Alice\", 34, \"Engineer\"),\n",
    "    (\"Bob\", 45, \"Manager\"),\n",
    "    (\"Charlie\", 29, \"Analyst\"),\n",
    "    (\"David\", 38, \"Engineer\")\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"job\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# HDFS에서 CSV 읽기 예시 (파일이 존재할 경우)\n",
    "# df_hdfs = spark.read.csv(\"hdfs://hdfs-cluster/user/data/sample.csv\", header=True)\n",
    "# df_hdfs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b7483",
   "metadata": {},
   "source": [
    "# 데이터 변환 및 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d07deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이가 35세 이상인 사람 필터링\n",
    "df_filtered = df.filter(df.age > 35)\n",
    "df_filtered.show()\n",
    "\n",
    "# 새로운 열 추가 (나이 + 10)\n",
    "from pyspark.sql.functions import col\n",
    "df_transformed = df.withColumn(\"age_plus_10\", col(\"age\") + 10)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0abb19",
   "metadata": {},
   "source": [
    "# 집계 및 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5083ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직업별 평균 나이 계산\n",
    "from pyspark.sql.functions import avg\n",
    "df_grouped = df.groupBy(\"job\").agg(avg(\"age\").alias(\"avg_age\"))\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3766e3",
   "metadata": {},
   "source": [
    "# 데이터 저장 및 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd06dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS에 CSV로 저장\n",
    "df_grouped.write.csv(\"hdfs://hdfs-cluster/user/data/output.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# PostgreSQL에 저장 (JDBC 사용)\n",
    "df_grouped.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres-db-rw:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"example_table\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"your_password\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data saved to HDFS and PostgreSQL\")\n",
    "\n",
    "# SparkSession 종료\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
